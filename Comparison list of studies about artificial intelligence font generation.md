| **Name**                              | Bib                                          | **Tasks**                                                                           | **Input types**                                       | **Encoding Representation**                                                         | **Decoding Modality**        | **Output types**                   | **Representation**                                          | **Latent Space**                                           | **Datasets Size**                                                                      | **Training / Testing Distribution**                                                                                                                        | **Dataset Source**                                     | **Techniques and Features**                                                                                     | **Architecture Base**                  | **Layers**                                                                                         | **Output Evaluation Methods**                                                                     | **Evaluation Comparison**                                 | **Type Designer Involvement as expert** |
| ------------------------------------- | -------------------------------------------- | ----------------------------------------------------------------------------------- | ----------------------------------------------------- | ----------------------------------------------------------------------------------- | ---------------------------- | ---------------------------------- | ----------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------- | --------------------------------------- |
| **VecFusion**                         | [@vikasthamizharasanVecFusionVectorFont2023] | interpolation, completion                                                           | Raster images of glyphs                               | Cascaded diffusion model with vector diffusion                                      | Sequential                   | Vector glyphs                      | Mixed discrete-continuous representation for control points | Not specified                                              | 1,424 fonts, 577 distinct Unicode glyphs                                               | Train: 314K glyphs, Val: 5K glyphs, Test: 5K glyphs                                                                                                        | Google Fonts                                           | Cascaded diffusion model, Mixed discrete-continuous representation, Transformer-based vector model              | Transformer-based                      | 8 Transformer layers                                                                               | L1, Chamfer Distance (CD), Control point difference (#cp diff), Vector path difference (#vp diff) | ChiroDiff, DeepVecFont-v2                                 | No                                      |
| **VQ-font**                           | [@VQFontFewShot2023]                         | Font completion                                                                     | Raster images of glyphs                               | Similarity-guided global style and quantization local style representation          | Sequential                   | Font glyphs                        | Discrete latent codes for component-level styles            | Not specified                                              | 386 Chinese fonts, 3,500 characters each                                               | Train: 370 fonts, 3,000 characters each; Test: 15 unseen fonts, 3,000 seen characters each, 15 unseen fonts, 500 unseen characters each                    | Custom dataset                                         | Global and local style aggregation, Cross-attention-based style transfer, GAN-based training                    | GAN and VQ-VAE                         | 3 Transformer layers for cross-attention, 8 attention heads                                        | SSIM, RMSE, LPIPS, FID, User Study                                                                | FUNIT, MX-Font, LF-Font, DG-Font, AGIS-net, FS-Font       | No                                      |
| **Joint Implicit Neural**             | [@JointImplicitJointImplicit2023]            | interpolation                                                                       | Pixelated font images                                 | Joint neural representation using SDF and probabilistic corner field (CF)           | Sequential                   | Vector fonts                       | Embeddings of SDF and CF                                    | Not specified                                              | 1,425 fonts, 52 glyphs per font                                                        | Train: 90%, Test: 10%                                                                                                                                      | Public datasets                                        | Implicit neural representation, Corner field modeling, Dual contouring for vectorization                        | HyperNetworks-based                    | Multi-layer perceptrons (MLPs) for SDF and CF networks                                             | L1 error, SSIM, s-mIoU                                                                            | Im2Vec, Multi-Implicit, DeepVecFont, Attr2Font            | No                                      |
| **Consistent Typography Generation**  | [@DiverseConsistentTypography2023]           | style transfer                                                                      | Graphic documents with text elements                  | Autoregressive Transformer with attention mechanism                                 | Sequential                   | Typographic designs                | Fine-grained typographic attributes                         | Not specified                                              | 23,475 design templates                                                                | Train: 18,780, Test: 2,347, Val: 2,347                                                                                                                     | Crello dataset                                         | Structure-preserved sampling, Fine-grained attribute generation, Consistency and diversity in styling           | Transformer-based                      | 8 Transformer blocks                                                                               | Attribute metrics (accuracy, MAE, color difference), Structure score, Diversity score             | CanvasVAE, MFC                                            | No                                      |
| **DualVector**                        | [@DualVectorUnsupervisedVector2023]          | interpolation, font completion                                                      | Glyph images                                          | Joint vector and pixel representation                                               | Sequential                   | Vector glyphs                      | Dual-part vector representation                             | Shared between modalities                                  | Public datasets: 1,425 fonts, 52 glyphs per font                                       | Train: 90%, Test: 10%                                                                                                                                      | Public datasets                                        | Dual-part representation, Differentiable rendering, Contour refinement, UDF initialization                      | Transformer-based                      | Encoder: CNN + Transformer (6 layers), Decoder: Transformer + MLP                                  | SSIM, L1, s-IoU, LPIPS                                                                            | DeepVecFont, Im2Vec, Multi-Implicits                      | No                                      |
| **Contour Completion**                | [@ContourCompletionTransformers2023]         | contour completion                                                                  | Contour sequences with missing points                 | Sequence embedding                                                                  | Sequence-to-sequence         | Completed contour sequences        | 5D vectors (x, y, Contour ID, Point ID, curve flag)         | Not specified                                              | Google Fonts: 489 Serif, 1,275 Sans-Serif, 327 Display, 91 Handwriting                 | Train: 1,777 fonts, Val: 200 fonts, Test: 205 fonts                                                                                                        | Google Fonts                                           | Multi-task learning, Loss functions for contour, point, coordinate, and flags                                   | Transformer-based                      | 4 Transformer layers (Encoder & Decoder)                                                           | L1 distance, Hausdorff distance                                                                   | Standard Transformer-Encoder                              | No                                      |
| **DeepVecFont-v2**                    | [@DeepVecFontv2ExploitingTransformers2023]   | interpolation, font completion                                                      | Raster images and vector outlines                     | Relaxation representation for vector outlines                                       | Sequence-to-sequence         | Vector glyphs                      | Embeddings of drawing commands and coordinates              | Not specified                                              | English: 8,035 fonts, Chinese: 212 fonts                                               | Train: 8,035 (EN), 212 (CN), Test: 1,425 (EN), 34 (CN)                                                                                                     | Google Fonts                                           | Transformer encoder-decoder, Bezier curve alignment, Self-refinement                                            | Transformer-based                      | 6 Transformer layers                                                                               | Reconstruction errors (L1), IoU, Bezier curve alignment loss                                      | DeepSVG, DeepVecFont                                      | No                                      |
| **VecFontSDF**                        | [@VecFontSDFLearningReconstruct2023]         | style transfer                                                                      | Raster images of glyphs                               | Spatial representation                                                              | Sequential                   | Font glyphs                        | Signed Distance Function (SDF)                              | Multi-scale                                                | Google Fonts: 143K glyph images                                                        | Train: 90%, Test: 10%                                                                                                                                      | Google Fonts                                           | SDF rendering, Style transfer, Multi-scale feature extraction                                                   | CNN-based                              | 16 convolutional layers                                                                            | MSE, SSIM, FID                                                                                    | DeepSVG, Im2vec, DeepVecFont                              | No                                      |
| **DS-Font**                           | [@DSFontFewshotFont2023]                     | Font completion                                                                     | Raster images of glyphs                               | Style representation through multi-layer style projector (MSP)                      | Sequential                   | Font glyphs                        | Embeddings of style codes                                   | Not specified                                              | 1,425 fonts, 52 glyphs per font                                                        | Train: 90%, Test: 10%                                                                                                                                      | Public datasets                                        | Multi-layer style projector (MSP), multi-task patch discriminator, contrastive learning                         | GAN-based                              | Multi-layer style projector, multi-task patch discriminator, generator with attention mechanism    | L1 loss, LPIPS, RMSE, Acc(C), Acc(S), FID(C), FID(S)                                              | LF-Font, MX-Font, DG-Font, FTransGAN, MF-Net              | No                                      |
| **SVGformer**                         | [@SVGformerRepresentationLearning2023]       | Reconstruction, Classification, Interpolation, Retrieval                            | Continuous SVG commands                               | Sequential and Geometric representation                                             | Sequential                   | Vector graphics (SVGs)             | Embeddings of continuous commands and geometric information | Not specified                                              | Google Fonts, Icons dataset: Size not specified                                        | Not specified                                                                                                                                              | Google Fonts, Icons datasets                           | Geometric self-attention, Graph convolutional network (GCN), Continuous value embedding                         | Transformer-based                      | Encoder: Multiple geometric self-attention modules, Decoder: Multi-head attention                  | Chamfer distance (CD), Cross-entropy (CE)                                                         | DeepSVG, LayoutTransformer                                | No                                      |
| **Character-Aware**                   | [@CharacterAwareModelsImprove2022]           | Text rendering, Spelling                                                            | Text inputs (character-level and token-level)         | Character-aware and character-blind representations                                 | Sequential                   | Visual text rendering in images    | Token and character embeddings                              | Not specified                                              | Not specified                                                                          | Train: 500,000 steps, Test: Not specified                                                                                                                  | Laion-400M                                             | Character-aware and character-blind text encoders, Hybrid models, Pretraining                                   | T5, ByT5, Concat(T5-XXL, ByT5-Small)   | Multiple layers (sizes not specified)                                                              | OCR-based metrics (accuracy), Human ratings                                                       | Imagen, Stable Diffusion, Parti                           | No                                      |
| **GAS-NeXt**                          | [@GASNeXtFewShotCrossLingual2022]            | Few-shot cross-lingual font generation                                              | Raster images and style reference images              | Layer attention and context-aware attention                                         | Sequential                   | Stylized glyph images              | Encoded style and content features                          | Not specified                                              | Li et al. dataset                                                                      | Train: 90%, Test: 10%                                                                                                                                      | Li et al. dataset, Azadi et al. dataset, CASIA dataset | Layer attention, Context-aware attention, Local discriminator                                                   | AGIS-Net and Font Translator GAN based | Encoder: 6 convolutional layers, Decoder: 6 deconvolutional layers                                 | FID, SSIM, Pixel-level Accuracy                                                                   | Font Translator GAN, AGIS-Net                             | No                                      |
| **Diff-Font**                         | [@DiffFontDiffusionModel2022]                | font completion                                                                     | Raster images of glyphs                               | Gaussian noise representation                                                       | Sequential                   | Font glyphs                        | Character attributes embedding (content, stroke, style)     | Not specified                                              | Small: 1,000 Chinese characters, Large: 3,755 Chinese characters                       | Train: 80%, Test: 20%                                                                                                                                      | Custom dataset                                         | Diffusion model, Stroke-wise information                                                                        | UNet-based DDPM                        | Multi-scale U-Net layers                                                                           | SSIM, RMSE, LPIPS, FID                                                                            | FUNIT, MX-Font, DG-Font                                   | No                                      |
| **Neural Font Rendering**             | [@NeuralFontRendering2022]                   | Reconstruction, Interpolation                                                       | Vector glyph outlines                                 | Implicit neural representation                                                      | Sequential                   | Rasterized glyphs at various sizes | Implicit functions                                          | Not specified                                              | Not specified                                                                          | Not specified                                                                                                                                              | Custom dataset                                         | Implicit neural representation, Frequency encoding, Batch normalization adaptations                             | U-Net-based, Implicit model            | Encoder-decoder with multiple layers, Implicit model with 5 layers                                 | L2 pixelwise loss, Focal loss                                                                     | Comparison between masked MLP and implicit model          | No                                      |
| **Paired-glyph**                      | [@FontRepresentationLearning2022]            | Font retrieval, Style transfer, Completion                                          | Glyph images                                          | Paired-glyph matching learning                                                      | Sequential                   | Font embeddings                    | Embeddings of glyph representations                         | Not specified                                              | O’Donovan: 1,088 fonts, Capitals64: 10,682 fonts, Open Font Library (OFL): 3,802 fonts | O’Donovan: Train: 1,088 fonts, Val: 28 fonts, Capitals64: Train: 7,649 fonts, Val: 1,473 fonts, Test: 1,560 fonts, OFL: Train: 3,702 fonts, Val: 100 fonts | O’Donovan dataset, Capitals64, OFL                     | Paired-glyph matching, Cross-entropy loss, L1 loss, Contrastive learning                                        | ResNet18-based                         | ResNet18 backbone, additional layers for projection head                                           | Retrieval mean accuracy (MACC_Ret), Font attribute prediction (L1-error)                          | Classification, Style Transfer, Autoencoder               | No                                      |
| **FS-Font**                           | [@FSFontFewShotFont2022]                     | Font completion                                                                     | Raster images of glyphs                               | Fine-grained local style representation through cross-attention                     | Sequential                   | Stylized glyph images              | Fine-grained local style representation (FLS)               | Not specified                                              | 407 fonts, 3,396 characters                                                            | Train: 397 fonts, 2,896 characters, Test: 10 fonts, 500 unseen characters (UFUC), 2,896 seen characters (UFSC)                                             | Custom dataset                                         | Cross-attention based style aggregation, Self-reconstruction branch, Reference selection strategy               | Convolutional and Residual Blocks      | Multiple convolutional and residual layers                                                         | L1 loss, RMSE, SSIM, LPIPS, User Study                                                            | FUNIT, DG-Font, MX-Font, AGIS-net, LF-Font                | No                                      |
| **GenText**                           | [@GenTextUnsupervisedArtistic2022]           | text effect transfer, font style transfer, image style transfer, font interpolation | Content images, font images, texture reference images | Spatial and global code representation                                              | Sequence-to-sequence         | Artistic text images               | Embeddings (spatial and global code)                        | Not specified                                              | Artistic text benchmarks (e.g., TE141K)                                                | Train: 90%, Test: 10%                                                                                                                                      | Public datasets                                        | GAN, unsupervised learning, stylization and destylization                                                       | Encoder-decoder GAN                    | 4 downsampling residual blocks, 2 convolution layers                                               | PSNR, SSIM, Perceptual loss, Style loss                                                           | AdaIN, Dpatch, NCE, CycleGAN                              | No                                      |
| **SVGVecFont**                        | [@SVGVecFontSVGVector2022]                   | interpolation, completion                                                           | SVG vector graphics                                   | Sequence-to-sequence                                                                | Sequence-to-sequence         | Vector glyphs                      | Commands and Coordinates                                    | Not specified                                              | Chinese: 407 fonts, 3396 characters                                                    | Train: 397 fonts, Test: 10 fonts                                                                                                                           | Public datasets                                        | Transformer encoder-decoder, Style Aggregation Module                                                           | Transformer-based                      | Multi-head projection, Self-reconstruction branch                                                  | L1 loss, Adversarial loss, SSIM, LPIPS, FID                                                       | DeepVecFont, Diff-Font, GAS-NeXt                          | No                                      |
| **Impression Labels**                 | [@FontGenerationMissing2022]                 | Style transfer                                                                      | Raster images of glyphs                               | Conditional GAN with missing label handling                                         | Sequential                   | Font glyphs                        | Impression label embeddings                                 | Not specified                                              | MyFonts: 17,202 fonts, 1,430 impression labels                                         | Train: 90%, Test: 10%                                                                                                                                      | MyFonts dataset                                        | Co-occurrence-based missing label estimator, Impression label space compressor, Style consistency discriminator | GAN-based                              | Progressive GAN with auxiliary classifiers                                                         | FID, Intra-FID, mAP-train, mAP-test                                                               | C-GAN+, AC-GAN+, CP-GAN+, Imp2Font                        | No                                      |
| **FontNet**                           | [@FontNetClosingGap2022]                     | font style transfer, font completion                                                | Font images                                           | Embedding                                                                           | Adversarial generation       | High-resolution font images        | Embeddings of style and content features                    | Style embedding space                                      | 90 Korean fonts, 2,350 characters                                                      | Train: 75% fonts, Test: 25% fonts                                                                                                                          | Naver Fonts                                            | StyleGAN, Separator network, Triplet loss                                                                       | StyleGAN-based                         | Encoder-Decoder with separator network                                                             | SSIM, mFID, Top-1 accuracy                                                                        | MX-Font, FUNIT                                            | No                                      |
| **SE-GAN**                            | [@SEGANSkeletonEnhanced2022]                 | style transfer                                                                      | Raster images of brush handwriting                    | Spatial representation                                                              | Sequential                   | Font glyphs                        | Skeleton-based representation                               | Multi-scale                                                | Custom dataset: 15,799 high-resolution images                                          | Train: 80%, Dev: 10%, Test: 10%                                                                                                                            | Custom dataset                                         | GAN-based model, Self-attentive Refined Attention Module (SAttRAM), skeleton discriminator                      | GAN-based                              | 4 residual blocks for each encoder, 2 discriminators                                               | Content accuracy, FID, User preference                                                            | zi2zi, CycleGAN, StarGAN, DF-Font                         | Yes                                     |
| **Arbitrary Font Generation**         | [@ArbitraryFontGeneration2022]               | style transfer                                                                      | Raster images of fonts                                | Spatial representation                                                              | Sequential                   | Font images                        | Disentangled features of text content and font style        | N/A                                                        | Korean: 238 train, 40 test; Chinese: 185 train, 15 test; English: 185 train, 15 test   | Train: 85%, Test: 15%                                                                                                                                      | Noonnu, Internet, Chinese Font Design                  | Stacked input, Consistency loss, VGG-19, AdaIN, Hallucinated stack                                              | VGG-19 based                           | VGG-19 layers, AdaIN layers, ConvBlock layers                                                      | FID, L1 distance, Perceptual distance                                                             | FUNIT, EMD                                                | No                                      |
| **CVFont**                            | [@CVFontSynthesizingChinese2022]             | style transfer                                                                      | Raster images of glyphs                               | Hierarchical approach to component extraction, using CPD algorithm for registration | Sequential                   | Chinese vector fonts               | Components and strokes                                      | Multi-scale                                                | GB2312 Kaiti font library (6763 characters), 70 Chinese fonts                          | Pre-train: 60 fonts, Online: 10 fonts, Testing: 69 fonts                                                                                                   | Founder Group                                          | Layout prediction with Faster R-CNN, U-Net based generator, Skeleton extraction, Shape decomposition            | ResNet-101 + U-Net                     | ResNet-101 backbone, U-Net generator with 3 downsampling layers, Faster R-CNN detector             | IoU scores, Qualitative user study                                                                | Rewrite, pix2pix, CycleGAN, EasyFont, zi2zi               | Yes                                     |
| **XMP-Font**                          | [@XMPFontSelfSupervisedCrossModality2022]    | Font completion                                                                     | Glyph images, stroke labels                           | Cross-modality representation                                                       | Cross-modality               | Glyphs                             | Stroke-level, component-level, character-level styles       | Pre-trained with self-supervised signals                   | 100 font styles                                                                        | Train: 90%, Test: 10%                                                                                                                                      | Founder font libraries                                 | Cross-modality transformer-based encoder, ECA modules, LSTM-based stroke loss                                   | Transformer-based                      | Cross-modality encoder with BERT layers, 4 ECA modules                                             | FID, PSNR, SSIM, L1, User study                                                                   | StarGAN-v2, FUNIT, LF-Font, MX-Font, DG-Font              | No                                      |
| **Noisy Impressions**                 | [@SharedLatentSpace2022]                     | Cross-modal: shape-to-impression, impression-to-shape                               | Glyph images, impression words                        | Shared latent space representation                                                  | Cross-modal                  | Glyph images, impression words     | Vector representations for font shapes and impression words | Shared latent space with DeepSets                          | MyFonts: 18,815 fonts                                                                  | Train: 9,980, Validation: 2,992, Test: 1,223                                                                                                               | MyFonts dataset                                        | DeepSets for shape-relevant impression filtering, cross-modal autoencoders                                      | Autoencoder, ResNet18-based encoder    | ResNet18 encoder, deconvolutional layers for decoder                                               | Precision@K, average retrieval rank, Hausdorff distance                                           | Impressions2Font                                          | No                                      |
| **CKFont**                            | [@CKFontFewShotKorean2021]                   | Font completion                                                                     | Glyph images                                          | Spatial representation                                                              | Sequential                   | Glyphs                             | Character-level styles                                      | Pre-trained with self-supervised signals                   | 2,350 glyphs                                                                           | Train: 2,000, Test: 350                                                                                                                                    | Hangul standard code system (KS X 1001)                | Conditional GAN, dual encoders, style extraction from components                                                | GAN-based                              | 6 convolutional layers in each encoder, 6 deconvolutional layers in the decoder                    | L1, L2, SSIM, FID                                                                                 | zi2zi, SKFont                                             | No                                      |
| **Dual Latent Manifolds**             | [@ScalableFontReconstruction2021]            | Reconstruction                                                                      | Glyph images                                          | Dual latent manifolds representation                                                | Matrix factorization         | Glyph images                       | Vector representations for font style and character shape   | Dual latent spaces with Gaussian priors                    | Google Fonts: 2017 fonts, Chinese Simplified: 623 fonts                                | Train: 60%, Dev: 20%, Test: 20%                                                                                                                            | Google Fonts, Chinese Simplified dataset               | Dual manifold model, adaptive wavelet loss, U-Net architecture                                                  | U-Net-based                            | Encoder: convolutional layers, Decoder: transposed convolutional layers with MLP parameters        | SSIM, L2, Human evaluation (Amazon Mechanical Turk)                                               | EMD, Nearest Neighbor                                     | No                                      |
| **ZiGAN**                             | [@ZiGANFinegrainedChinese2021]               | Style transfer                                                                      | Glyph images                                          | Spatial representation                                                              | Sequential                   | Glyphs                             | Character-level styles                                      | Learned correlation between structures of different styles | 61,151 images, 6560 characters                                                         | Train: 100 or 200 shots, Test: remaining images                                                                                                            | Chinese calligraphy character website                  | GAN-based, CAM attention module, CycleGAN                                                                       | GAN-based                              | Encoder: 8 convolutional layers, Decoder: 8 deconvolutional layers                                 | FID, IOU, Top-1 Accuracy, User study                                                              | zi2zi, pix2pix, U-GAT-IT, CycleGAN, StarGAN, CalliGAN     | No                                      |
| **DeepVecFont**                       | [@DeepVecFontSynthesizingHighquality2021]    | Few-shot font generation, interpolation                                             | Raster images, vector outlines                        | Dual-modality representation                                                        | Sequential                   | Vector glyphs                      | Image-aspect and sequence-aspect features                   | Latent space with Gaussian priors                          | 8K fonts for training, 1.5K for testing                                                | Train: 8K fonts, Test: 1.5K fonts                                                                                                                          | SVG-Fonts Dataset                                      | Dual-modality learning, differentiable rasterization, Mixture Density Network                                   | CNN-based, RNN-based                   | Convolutional layers, LSTM layers                                                                  | L1 loss, Perceptual loss, Cross-Entropy loss, MDN loss                                            | SVG-VAE, DeepSVG, Im2Vec                                  | No                                      |
| **CycleFont**                         | [@FontCompletionManipulation2021]            | Font completion, interpolation, manipulation                                        | Raster images, SVG curves, point sets                 | Multi-modality representation (image-to-graph-to-image)                             | Multi-modality (graph-based) | Glyph images                       | Graph-based representation                                  | Latent space with graph representation                     | Google Fonts: 2693 fonts, 55,554 glyphs                                                | Train: 95%, Test: 5%                                                                                                                                       | Google Fonts dataset                                   | Cross-modality auto-encoder, graph-based representation                                                         | Graph-based, auto-encoder              | Image encoder (Conv2D), point set decoder, graph constructor, neural renderer                      | MSE, PSNR, SSIM                                                                                   | TCN                                                       | No                                      |
| **Implicit Glyph Shape**              | [@LearningImplicitGlyph2021]                 | Font reconstruction, interpolation, style transfer                                  | Raster images                                         | Implicit representation                                                             | Implicit function decoding   | Glyph images                       | Signed distance functions with quadratic curves             | Continuous 2D space                                        | 26390 glyph images                                                                     | Train: 26390 images, Validation: dataset split                                                                                                             | Custom dataset                                         | Implicit representation, quadratic curves, disentangled encoder, auxiliary character classifier                 | CNN-based, ResNet-18                   | ResNet-18 encoder, MLP for curve parameters                                                        | SSIM, LPIPS, L1 distance, User study                                                              | VAE, pix2pix, AGIS-Net, FANnet                            | No                                      |
| **Perceptual Manifold of Fonts**      | [@LearningPerceptualManifold2021]            | Font exploration, style transfer                                                    | Raster images                                         | Latent space representation                                                         | Sequential                   | Glyph images                       | Perceptual manifolds                                        | Continuous latent space                                    | Google Fonts: 2169 fonts                                                               | Train: 90%, Test: 10%                                                                                                                                      | Google Fonts                                           | Variational Autoencoder (VAE), t-SNE for manifold learning, kernel density estimation                           | VAE-based                              | Encoder: 4 convolutional layers, Decoder: 2 convolutional layers                                   | SSIM, User study                                                                                  | Comparison with traditional font exploration interfaces   | Yes                                     |
| **FontRL**                            | [@FontRLChineseFont2021]                     | Font completion                                                                     | Glyph images                                          | Spatial representation                                                              | Sequential                   | Glyph images                       | Stroke-level styles                                         | Deep reinforcement learning with TPS transformations       | 6,763 characters in 5 styles                                                           | Train: 775 characters, Test: 5 styles                                                                                                                      | Custom dataset                                         | Deep reinforcement learning, TPS transformation, CNN-based image rendering                                      | ResNet-18, CNN-based                   | Policy network (ResNet-18), BBoxNet (ResNet-34), StyleNet for rendering                            | L1 loss, IoU, User study                                                                          | SCFont, zi2zi, DCFont, FontRNN, pix2pix                   | No                                      |
| **LF-Font**                           | [@LFFontFewshotFont2021]                     | Font completion                                                                     | Glyph images                                          | Spatial representation                                                              | Sequential                   | Glyphs                             | Localized style representations, factorization modules      | Factorized component and style factors                     | 19,514 characters                                                                      | Train: 467 fonts, Test: 15 fonts                                                                                                                           | Custom collected Chinese fonts                         | Component-wise style encoding, content encoder, factorization modules                                           | CNN-based                              | Style encoder, content encoder, generator with 4 convolutional layers                              | LPIPS, Accuracy, FID, User study                                                                  | SA-VAE, EMD, AGIS-Net, FUNIT, DM-Font                     | No                                      |
| **StrokeGAN**                         | [@StrokeGANReducingMode2021]                 | Style transfer                                                                      | Glyph images                                          | Stroke encoding representation                                                      | Sequential                   | Glyph images                       | Stroke-level encoding                                       | CycleGAN with stroke encoding                              | 9 different fonts datasets                                                             | Train: 90%, Test: 10%                                                                                                                                      | Custom collected, CASIA-HWDB1.1, Internet              | Stroke encoding, CycleGAN, stroke-encoding reconstruction loss                                                  | CycleGAN-based                         | 2 convolutional layers (down-sampling), 9 residual modules, 2 deconvolutional layers (up-sampling) | Content accuracy, Recognition accuracy, Stroke error                                              | CycleGAN, zi2zi, Chinese Typography Transfer (CTT)        | No                                      |
| **DG-Font**                           | [@DGFontDeformableGenerative2021]            | Font completion                                                                     | Raster images of glyphs                               | Spatial representation                                                              | Sequential                   | Font glyphs                        | Spatial features and latent vector representations          | Deformable                                                 | Custom dataset: 410 fonts, 990 characters each                                         | Train: 400 fonts, 800 chars/font; Test: 10 fonts, 190 chars/font                                                                                           | Custom dataset                                         | Deformable convolution, Feature Deformation Skip Connection (FDSC), AdaIN                                       | CNN-based                              | Deformable convolutions, residual blocks, multi-task discriminator                                 | L1 loss, RMSE, SSIM, LPIPS, FID                                                                   | CycleGAN, EMD, Zi2zi, GANimorph, FUNIT                    | No                                      |
| **Radical Composition Network (RCN)** | [@RadicalCompositionNetwork2021]             | Cross-modal: Caption-to-font                                                        | Caption text, Raster images of radicals               | Sequential                                                                          | Sequential                   | Chinese character images           | Embeddings                                                  | Tree-structured                                            | Printed: 114,665 characters; Handwritten: 2,674,784 characters                         | Train: 22,933 classes, Test: 4,172 classes                                                                                                                 | Custom dataset                                         | Tree-structured encoder (TRN), DenseNet, Deconvolution, Perceptual loss                                         | Encoder-decoder (CNN-based)            | Tree-structured recurrent units, Deconvolution layers                                              | L1 loss, RMSE, SSIM, LPIPS                                                                        | RAN, zi2zi                                                | No                                      |
| **SkelGAN**                           | [@SkelGANFontImage2021]                      | Skeletonization and style transfer                                                  | Font images                                           | Spatial representation                                                              | Sequential                   | Skeletons of font glyphs           | One-pixel-width skeletons                                   | Multi-scale                                                | Custom dataset: 70,500 glyph images                                                    | Train: 90%, Test: 10%                                                                                                                                      | Custom dataset                                         | GANs, Style classification, Character classification, Multi-scale feature extraction                            | Modified U-Net                         | 14 layers (7 encoder, 7 decoder)                                                                   | SSIM, L1 distance                                                                                 | Lee’s method, Pix2pix                                     | No                                      |
| **FTransGAN**                         | [@chenhaoFTransGANPyTorch2023]               | style transfer, font completion                                                     | Grayscale images of glyphs                            | Multi-level attention with Context-aware and Layer Attention Networks               | Sequential                   | Grayscale glyphs                   | Local and global feature representation                     | Not specified                                              | 847 fonts, each with 52 English letters and ~1000 Chinese characters                   | Train: 847 fonts, Test: 29 fonts                                                                                                                           | Constructed by authors                                 | Multi-level attention network, Context-aware Attention Network, Layer Attention Network                         | GAN-based                              | Encoder-Decoder structure with 3 Conv layers, 6 ResNet blocks, and 3 up-conv layers                | L1 loss, Style loss, Content loss, MAE, SSIM, MS-SSIM, mFID, Top-1 accuracy                       | EMD, DFS                                                  | No                                      |
| **DeepSVG**                           | [@DeepSVGHierarchicalGenerative2020]         | interpolation, animation, manipulation                                              | SVG commands                                          | Hierarchical Transformer-based architecture                                         | Non-autoregressive           | SVG images                         | Discrete continuous embedding                               | Not specified                                              | 100,000 high-quality icons                                                             | Train/Test split not specified                                                                                                                             | SVG-Icons8, constructed by authors                     | Hierarchical generative network, Non-autoregressive feed-forward model                                          | Transformer-based                      | 4 layers in encoder and decoder with 512 feed-forward dimension                                    | Chamfer distance, Reconstruction Error (RE), Interpolation Smoothness (IS)                        | One-stage autoregressive, One-stage feed-forward, SVG-VAE | No                                      |
| **Font2Fonts**                        | [@Font2FontsModifiedImagetoImage2020]        | style transfer, font completion                                                     | Grayscale images of glyphs                            | Conditional GAN with multi-domain translation                                       | Sequential                   | Grayscale glyphs                   | Local and global feature representation                     | Not specified                                              | 20 Korean fonts, 2,350 most commonly used Korean Hangul characters                     | Train: 75% (15 fonts), Test: 25% (5 fonts)                                                                                                                 | Constructed by authors                                 | Conditional GAN, Multi-domain translation, Unicode-based font dataset generator                                 | GAN-based                              | 7 down-sampling and 7 up-sampling layers with Instance Normalization                               | L1 loss, GAN loss, Style classification loss, MAE, SSIM                                           | pix2pix                                                   | No                                      |
| **CalliGAN**                          | [@CalliGANStyleStructureaware2020]           | style transfer, generation                                                          | Grayscale images of glyphs                            | Component-based encoder-decoder with multi-domain translation                       | Sequential                   | Grayscale glyphs                   | Embedding and component encoding                            | Not specified                                              | 29 calligraphy styles, 47552 images                                                    | Train: 39815 images, Test: 7737 images                                                                                                                     | Constructed by authors                                 | Component-based encoder, U-Net-based generator, multi-domain translation                                        | GAN-based                              | 8 encoder and 8 decoder layers, Component encoder with LSTM                                        | MSE, SSIM, Human subject study                                                                    | zi2zi, AEGG                                               | No                                      |







